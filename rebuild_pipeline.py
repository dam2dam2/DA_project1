import pandas as pd
import numpy as np
import os
import glob
import re
import hashlib
import json

def generate_uid(row):
    try:
        val = str(row['íŒë§¤ë‹¨ê°€']).replace(',', '')
        unit_price = str(int(float(val)))
    except:
        unit_price = "0"
    
    # UID = hash(ìƒí’ˆì½”ë“œ + ìƒí’ˆëª… + ê³ ê°ì„ íƒì˜µì…˜ + ë‹¨ê°€)
    opt = str(row['ê³ ê°ì„ íƒì˜µì…˜']) if pd.notna(row['ê³ ê°ì„ íƒì˜µì…˜']) else ""
    text = f"{str(row['ìƒí’ˆì½”ë“œ'])}{str(row['ìƒí’ˆëª…'])}{opt}{unit_price}"
    return "U" + hashlib.md5(text.encode()).hexdigest().upper()[:5]

def anonymize_phone(phone_series):
    unique_phones = sorted(phone_series.unique().tolist())
    phone_map = {phone: f"ANON_{i+1:05d}" for i, phone in enumerate(unique_phones)}
    return phone_series.map(phone_map)

def get_next_version(directory, base_name="preprocessed_data"):
    files = glob.glob(os.path.join(directory, f"{base_name}_*.csv"))
    if not files:
        return 1
    versions = []
    for f in files:
        match = re.search(rf"{base_name}_(\d+)\.csv$", f)
        if match:
            versions.append(int(match.group(1)))
    return max(versions) + 1 if versions else 1

def load_manifest(directory):
    path = os.path.join(directory, "processed_files_manifest.json")
    if os.path.exists(path):
        try:
            with open(path, 'r') as f:
                return set(json.load(f))
        except:
            return set()
    return set()

def save_manifest(directory, manifest_set):
    path = os.path.join(directory, "processed_files_manifest.json")
    with open(path, 'w') as f:
        json.dump(sorted(list(manifest_set)), f, indent=4)

def rebuild_pipeline():
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    ORIGIN_DIR = os.path.join(BASE_DIR, "data", "origin_data")
    DATA_DIR = os.path.join(BASE_DIR, "data")
    
    LATEST_OLD_PATH = os.path.join(DATA_DIR, "preprocessed_data.csv")
    
    # 1. ê¸°ì¡´ ë°ì´í„° ë° ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í™•ì¸
    existing_df = pd.DataFrame()
    if os.path.exists(LATEST_OLD_PATH):
        print(f"   - ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘: {os.path.basename(LATEST_OLD_PATH)}")
        existing_df = pd.read_csv(LATEST_OLD_PATH)
    
    processed_files = load_manifest(DATA_DIR)
    
    # 2. ì‹ ê·œ íŒŒì¼ íƒìƒ‰
    all_excel_files = glob.glob(os.path.join(ORIGIN_DIR, "*.xlsx"))
    new_files = [f for f in all_excel_files if os.path.basename(f) not in processed_files]
    
    if not new_files:
        print("ğŸ’¡ ì¶”ê°€ëœ ì‹ ê·œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    print(f"1. ì‹ ê·œ íŒŒì¼ ê°€ê³µ ì‹œì‘: {len(new_files)}ê°œ íŒŒì¼")
    new_dfs = []
    for f in sorted(new_files):
        print(f"   - {os.path.basename(f)} ì½ëŠ” ì¤‘...")
        new_dfs.append(pd.read_excel(f))
    
    raw_new = pd.concat(new_dfs, ignore_index=True)
    
    # ì¤‘ë³µ ì œê±° (ì‹ ê·œ ë°ì´í„° ë‚´ì—ì„œë§Œ)
    if 'ì£¼ë¬¸ìƒí’ˆê³ ìœ ë²ˆí˜¸' in raw_new.columns:
        raw_new = raw_new.drop_duplicates(subset=['ì£¼ë¬¸ìƒí’ˆê³ ìœ ë²ˆí˜¸'], keep='first')

    print("2. ì‹ ê·œ ë°ì´í„° ìµëª…í™” ë° ì „ì²˜ë¦¬ ì§„í–‰ ì¤‘...")
    if 'ì£¼ë¬¸ìì—°ë½ì²˜' in raw_new.columns:
        raw_new['ì£¼ë¬¸ìì—°ë½ì²˜'] = anonymize_phone(raw_new['ì£¼ë¬¸ìì—°ë½ì²˜'].astype(str))
    if 'ìˆ˜ë ¹ì¸ì—°ë½ì²˜' in raw_new.columns:
        raw_new['ìˆ˜ë ¹ì¸ì—°ë½ì²˜'] = anonymize_phone(raw_new['ìˆ˜ë ¹ì¸ì—°ë½ì²˜'].astype(str))
    
    # ì˜µì…˜ì½”ë“œ ìƒì„± (ì‹ ê·œ ë°ì´í„° ê¸°ì¤€)
    if 'ìƒí’ˆì½”ë“œ' in raw_new.columns and 'ê³ ê°ì„ íƒì˜µì…˜' in raw_new.columns:
        df_opt = raw_new[['ìƒí’ˆì½”ë“œ', 'ê³ ê°ì„ íƒì˜µì…˜']].drop_duplicates().copy()
        df_opt['option_idx'] = df_opt.groupby('ìƒí’ˆì½”ë“œ').cumcount() + 1
        df_opt['ì˜µì…˜ì½”ë“œ'] = df_opt['ìƒí’ˆì½”ë“œ'].astype(str) + "_" + df_opt['option_idx'].astype(str)
        raw_new = pd.merge(raw_new, df_opt[['ìƒí’ˆì½”ë“œ', 'ê³ ê°ì„ íƒì˜µì…˜', 'ì˜µì…˜ì½”ë“œ']], on=['ìƒí’ˆì½”ë“œ', 'ê³ ê°ì„ íƒì˜µì…˜'], how='left')

    mapping = {
        'ê²°ì œê¸ˆì•¡(ìƒí’ˆë³„)': 'ê²°ì œê¸ˆì•¡',
        'ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡(ìƒí’ˆë³„)': 'ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡',
        'ìƒí’ˆê¸ˆì•¡(ì˜µì…˜í¬í•¨)': 'íŒë§¤ë‹¨ê°€',
        'ê³µê¸‰ê°€': 'ê³µê¸‰ë‹¨ê°€'
    }
    df = raw_new.rename(columns=mapping).copy()

    # ìˆ«ìí˜• ë³€í™˜
    numeric_cols = ['ê²°ì œê¸ˆì•¡', 'ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡', 'íŒë§¤ë‹¨ê°€', 'ê³µê¸‰ë‹¨ê°€', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ì·¨ì†Œìˆ˜ëŸ‰']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(float)

    # ê¸°ë³¸ ê³„ì‚°
    if 'íŒë§¤ë‹¨ê°€' in df.columns:
        df['íŒë§¤ë‹¨ê°€'] = df.apply(lambda r: r['íŒë§¤ë‹¨ê°€'] / r['ì£¼ë¬¸ìˆ˜ëŸ‰'] if r['ì£¼ë¬¸ìˆ˜ëŸ‰'] > 0 else r['íŒë§¤ë‹¨ê°€'], axis=1)
    df['ì‹¤ê²°ì œ ê¸ˆì•¡'] = df.get('ê²°ì œê¸ˆì•¡', 0) - df.get('ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡', 0)
    df['ì£¼ë¬¸-ì·¨ì†Œ ìˆ˜ëŸ‰'] = df.get('ì£¼ë¬¸ìˆ˜ëŸ‰', 0) - df.get('ì·¨ì†Œìˆ˜ëŸ‰', 0)
    df['ì·¨ì†Œì—¬ë¶€'] = df.get('ì·¨ì†Œìˆ˜ëŸ‰', 0).apply(lambda x: 'Y' if x > 0 else 'N')
    df['UID'] = df.apply(generate_uid, axis=1)

    # 3. ìƒì„¸ ë¶„ì„ í•„ë“œ ìƒì„±
    if 'ì£¼ë¬¸ì¼' in df.columns:
        df['ì£¼ë¬¸ì¼_dt'] = pd.to_datetime(df['ì£¼ë¬¸ì¼'])
        df['ì£¼ë¬¸ë‚ ì§œ'] = df['ì£¼ë¬¸ì¼_dt'].dt.date
    
    # ì§€ì—­ ë¶„ì„
    def extract_region(row):
        address = str(row.get('ì£¼ì†Œ', '')).strip()
        postcode = str(row.get('ìš°í¸ë²ˆí˜¸', '')).zfill(5)
        prefix = postcode[:2]
        
        sido_map = {
            '01':'ì„œìš¸','02':'ì„œìš¸','03':'ì„œìš¸','04':'ì„œìš¸','05':'ì„œìš¸','06':'ì„œìš¸','07':'ì„œìš¸','08':'ì„œìš¸','09':'ì„œìš¸',
            '10':'ê²½ê¸°','11':'ê²½ê¸°','12':'ê²½ê¸°','13':'ê²½ê¸°','14':'ê²½ê¸°','15':'ê²½ê¸°','16':'ê²½ê¸°','17':'ê²½ê¸°','18':'ê²½ê¸°','19':'ê²½ê¸°','20':'ê²½ê¸°',
            '21':'ì¸ì²œ','22':'ì¸ì²œ','23':'ì¸ì²œ','24':'ê°•ì›','25':'ê°•ì›','26':'ê°•ì›','27':'ì¶©ë¶','28':'ì¶©ë¶','29':'ì¶©ë¶','30':'ì„¸ì¢…',
            '31':'ì¶©ë‚¨','32':'ì¶©ë‚¨','33':'ì¶©ë‚¨','34':'ëŒ€ì „','35':'ëŒ€ì „','36':'ê²½ë¶','37':'ê²½ë¶','38':'ê²½ë¶','39':'ê²½ë¶','40':'ê²½ë¶',
            '41':'ëŒ€êµ¬','42':'ëŒ€êµ¬','43':'ëŒ€êµ¬','44':'ìš¸ì‚°','45':'ìš¸ì‚°','46':'ë¶€ì‚°','47':'ë¶€ì‚°','48':'ë¶€ì‚°','49':'ë¶€ì‚°',
            '50':'ê²½ë‚¨','51':'ê²½ë‚¨','52':'ê²½ë‚¨','53':'ê²½ë‚¨','54':'ì „ë¶','55':'ì „ë¶','56':'ì „ë¶','57':'ì „ë‚¨','58':'ì „ë‚¨','59':'ì „ë‚¨','60':'ì „ë‚¨',
            '61':'ê´‘ì£¼','62':'ê´‘ì£¼','63':'ì œì£¼'
        }
        formal_map = {
            'ì„œìš¸':'ì„œìš¸íŠ¹ë³„ì‹œ','ë¶€ì‚°':'ë¶€ì‚°ê´‘ì—­ì‹œ','ëŒ€êµ¬':'ëŒ€êµ¬ê´‘ì—­ì‹œ','ì¸ì²œ':'ì¸ì²œê´‘ì—­ì‹œ','ê´‘ì£¼':'ê´‘ì£¼ê´‘ì—­ì‹œ','ëŒ€ì „':'ëŒ€ì „ê´‘ì—­ì‹œ','ìš¸ì‚°':'ìš¸ì‚°ê´‘ì—­ì‹œ','ì„¸ì¢…':'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ',
            'ê²½ê¸°':'ê²½ê¸°ë„','ê°•ì›':'ê°•ì›íŠ¹ë³„ìì¹˜ë„','ì¶©ë¶':'ì¶©ì²­ë¶ë„','ì¶©ë‚¨':'ì¶©ì²­ë‚¨ë„','ì „ë¶':'ì „ë¶íŠ¹ë³„ìì¹˜ë„','ì „ë‚¨':'ì „ë¼ë‚¨ë„','ê²½ë¶':'ê²½ìƒë¶ë„','ê²½ë‚¨':'ê²½ìƒë‚¨ë„','ì œì£¼':'ì œì£¼íŠ¹ë³„ìì¹˜ë„'
        }
        
        sido = sido_map.get(prefix, next((s for s in sido_map.values() if s in address), "ê¸°íƒ€"))
        sido_formal = formal_map.get(sido, sido)
        
        # ì‹œêµ°êµ¬ ì¶”ì¶œ ê³ ë„í™”
        parts = address.split()
        sigungu = "ë¯¸ë¶„ë¥˜"
        
        if len(parts) > 0:
            # Sido í‚¤ì›Œë“œ ëª©ë¡
            sido_keywords = ['ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…', 'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼']
            formal_keywords = list(formal_map.values())
            all_sido_kws = sido_keywords + formal_keywords + ["ì „ë¶íŠ¹ë³„ìì¹˜ë„", "ê°•ì›íŠ¹ë³„ìì¹˜ë„", "ì œì£¼íŠ¹ë³„ìì¹˜ë„"]
            
            # ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ Sidoì¸ì§€ í™•ì¸
            is_first_sido = any(parts[0].startswith(kw) for kw in all_sido_kws)
            
            start_idx = 1 if is_first_sido else 0
            
            if len(parts) > start_idx:
                candidate = parts[start_idx]
                # 'ì‹œ', 'êµ°', 'êµ¬'ë¡œ ëë‚  ë•Œë§Œ ì‹œêµ°êµ¬ë¡œ ì¸ì •
                if candidate.endswith(('ì‹œ', 'êµ°', 'êµ¬')):
                    # 'ìˆ˜ì›ì‹œ íŒ”ë‹¬êµ¬'ì²˜ëŸ¼ ì‹œ+êµ¬ í˜•íƒœì¸ ê²½ìš° í•©ì¹¨
                    if candidate.endswith('ì‹œ') and len(parts) > start_idx + 1:
                        if parts[start_idx+1].endswith('êµ¬'):
                            candidate += " " + parts[start_idx+1]
                    sigungu = candidate
        
        if sido == "ì„¸ì¢…": sigungu = "ì„¸ì¢…ì‹œ"
            
        return sido, sido_formal, sigungu

    df[['ê´‘ì—­ì§€ì—­', 'ê´‘ì—­ì§€ì—­(ì •ì‹)', 'ì‹œêµ°êµ¬']] = df.apply(lambda r: pd.Series(extract_region(r)), axis=1)

    # í…ìŠ¤íŠ¸ ë¶„ì„
    def parse_text(row):
        txt = f"{str(row.get('ìƒí’ˆëª…', ''))} {str(row.get('ê³ ê°ì„ íƒì˜µì…˜', ''))}"
        
        # í’ˆì¢…(cat) ì„¸ë¶„í™” ë¡œì§
        if "í•œë¼ë´‰" in txt: cat = "í•œë¼ë´‰"
        elif "ë ˆë“œí–¥" in txt: cat = "ë ˆë“œí–¥"
        elif "ì²œí˜œí–¥" in txt: cat = "ì²œí˜œí–¥"
        elif "í™©ê¸ˆí–¥" in txt: cat = "í™©ê¸ˆí–¥"
        elif "ì¹´ë¼í–¥" in txt: cat = "ì¹´ë¼í–¥"
        elif any(k in txt for k in ["ì²­ê·¤", "í’‹ê·¤"]): cat = "ì²­ê·¤"
        elif any(k in txt for k in ["ê°ê·¤", "ì¡°ìƒ", "íƒ€ì´ë²¡", "ê·¤"]): cat = "ê°ê·¤"
        elif "ë‹¹ê·¼" in txt: cat = "ë‹¹ê·¼"
        elif "ê³ êµ¬ë§ˆ" in txt: cat = "ê³ êµ¬ë§ˆ"
        elif "ë¹„íŠ¸" in txt: cat = "ë¹„íŠ¸"
        elif "ì½œë¼ë¹„" in txt: cat = "ì½œë¼ë¹„"
        else: cat = "ê¸°íƒ€"

        sub = "íƒ€ì´ë²¡" if "íƒ€ì´ë²¡" in txt else ("ì¡°ìƒ" if "ì¡°ìƒ" in txt else ("í•˜ìš°ìŠ¤" if "í•˜ìš°ìŠ¤" in txt else "ì¼ë°˜"))
        size = "ê¸°íƒ€"
        if "ì†Œê³¼" in txt and "ì¤‘ì†Œê³¼" in txt: size = "ì†Œê³¼, ì¤‘ì†Œê³¼"
        elif "ì¤‘ëŒ€ê³¼" in txt and "ëŒ€ê³¼" in txt: size = "ì¤‘ëŒ€ê³¼, ëŒ€ê³¼"
        elif "ì†Œê³¼" in txt: size = "ì†Œê³¼"; 
        elif "í˜¼í•©" in txt: size = "í˜¼í•©"; 
        elif "ë¡œì–„ê³¼" in txt: size = "ë¡œì–„ê³¼"; 
        elif "ëŒ€ê³¼" in txt: size = "ëŒ€ê³¼"; 
        elif "ì¤‘ê³¼" in txt: size = "ì¤‘ê³¼";
        gift = "ì„ ë¬¼ì„¸íŠ¸" if any(k in txt for k in ["ì„ ë¬¼ì„¸íŠ¸", "ì„ ë¬¼ìš©"]) else "ê°€ì •ìš©"
        ev = "Y" if any(k in txt for k in ["ì´ë²¤íŠ¸", "1+1", "ë³´ì¥"]) else "N"
        # í”„ë¦¬ë¯¸ì—„ ë“±ê¸‰ ì‹ ê·œ ê¸°ì¤€ ì ìš©
        is_premium = False
        if gift == "ì„ ë¬¼ì„¸íŠ¸":
            is_premium = True
        elif cat == "ê°ê·¤" and size == "ë¡œì–„ê³¼":
            is_premium = True
        elif cat in ["í™©ê¸ˆí–¥", "í•œë¼ë´‰", "ë ˆë“œí–¥", "ì²œí˜œí–¥"] and any(s in size for s in ["ì¤‘ê³¼", "ì¤‘ëŒ€ê³¼", "ëŒ€ê³¼"]):
            is_premium = True
        
        grade = "í”„ë¦¬ë¯¸ì—„" if is_premium else "ì¼ë°˜"
        w_match = re.findall(r'(\d+(\.\d+)?)\s*(kg|KG)', txt)
        w = sum(float(m[0]) for m in w_match) if w_match else 0.0
        grp = "<3kg" if 0 < w < 3 else ("3-5kg" if w <= 5 else ("5-10kg" if w <= 10 else ">10kg")) if w > 0 else "ë¯¸ë¶„ë¥˜"
        purpose = "ì„ ë¬¼" if str(row.get('ì£¼ë¬¸ìëª…', '')).replace(' ','') != str(row.get('ìˆ˜ë ¹ì¸ëª…', '')).replace(' ','') or "ì„ ë¬¼" in txt else "ê°œì¸ì†Œë¹„"
        opt_clean = str(row.get('ê³ ê°ì„ íƒì˜µì…˜(íƒ€ì…ì œê±°)', row.get('ê³ ê°ì„ íƒì˜µì…˜', '')))
        return pd.Series([sub, cat, size, gift, w, grp, ev, grade, purpose, opt_clean])

    df[['ê°ê·¤ ì„¸ë¶€', 'í’ˆì¢…', 'ê³¼ìˆ˜ í¬ê¸°', 'ì„ ë¬¼ì„¸íŠ¸_ì—¬ë¶€', 'ë¬´ê²Œ(kg)', 'ë¬´ê²Œ êµ¬ë¶„', 'ì´ë²¤íŠ¸ ì—¬ë¶€', 'ìƒí’ˆì„±ë“±ê¸‰_ê·¸ë£¹', 'ëª©ì ', 'ê³ ê°ì„ íƒì˜µì…˜(íƒ€ì…ì œê±°)']] = df.apply(parse_text, axis=1)
    df['ê°€ê²©ëŒ€'] = df['ì‹¤ê²°ì œ ê¸ˆì•¡'].apply(lambda a: "1ë§Œì› ì´í•˜" if a <= 10000 else ("1~3ë§Œì›" if a <= 30000 else ("3~5ë§Œì›" if a <= 50000 else ("5~10ë§Œì›" if a <= 100000 else "10ë§Œì› ì´ˆë°˜"))))

    # 4. ì¬êµ¬ë§¤ íšŸìˆ˜ ì‚°ì¶œ (íˆìŠ¤í† ë¦¬ ì°¸ê³ )
    print("4. íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì¬êµ¬ë§¤ íšŸìˆ˜ ì‚°ì¶œ ì¤‘...")
    if not existing_df.empty:
        existing_df['ì£¼ë¬¸ì¼_temp'] = pd.to_datetime(existing_df['ì£¼ë¬¸ì¼']).dt.date
        history_map = existing_df.groupby('ì£¼ë¬¸ìì—°ë½ì²˜')['ì£¼ë¬¸ì¼_temp'].unique().to_dict()
        history_map = {k: set(v) for k, v in history_map.items()}
    else:
        history_map = {}

    new_history = df.groupby('ì£¼ë¬¸ìì—°ë½ì²˜')['ì£¼ë¬¸ë‚ ì§œ'].unique().to_dict()
    new_history = {k: set(v) for k, v in new_history.items()}

    def calc_repurchase(row):
        contact = str(row['ì£¼ë¬¸ìì—°ë½ì²˜'])
        order_date = row['ì£¼ë¬¸ë‚ ì§œ']
        past_dates = history_map.get(contact, set())
        current_new_dates = new_history.get(contact, set())
        combined_dates = past_dates.union(current_new_dates)
        return max(0, len(combined_dates) - 1)

    df['ì¬êµ¬ë§¤ íšŸìˆ˜'] = df.apply(calc_repurchase, axis=1)

    # 5. ìµœì¢… ë³‘í•© ë° ì €ì¥
    print("5. ë°ì´í„° ë³‘í•© ë° ì €ì¥ ì¤‘...")
    target_columns = [
        'UID', 'ì£¼ë¬¸ë²ˆí˜¸', 'ì£¼ë¬¸ì¼', 'ìƒí’ˆì½”ë“œ', 'ì˜µì…˜ì½”ë“œ', 'ìƒí’ˆëª…', 'ê³ ê°ì„ íƒì˜µì…˜', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ì·¨ì†Œìˆ˜ëŸ‰', 'ì£¼ë¬¸-ì·¨ì†Œ ìˆ˜ëŸ‰',
        'ê²°ì œê¸ˆì•¡', 'ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡', 'ì‹¤ê²°ì œ ê¸ˆì•¡', 'íŒë§¤ë‹¨ê°€', 'ê³µê¸‰ë‹¨ê°€', 'ì£¼ë¬¸ê²½ë¡œ', 'ì£¼ë¬¸ìëª…', 'ì…€ëŸ¬ëª…', 'ê²°ì œë°©ë²•',
        'ë°°ì†¡ì¤€ë¹„ ì²˜ë¦¬ì¼', 'ì…ê¸ˆì¼', 'ì£¼ë¬¸ìì—°ë½ì²˜', 'ì£¼ì†Œ', 'ì…ê¸ˆìëª…', 'ìˆ˜ë ¹ì¸ëª…', 'ìˆ˜ë ¹ì¸ì—°ë½ì²˜', 'íšŒì›êµ¬ë¶„', 'ìš°í¸ë²ˆí˜¸',
        'ê³ ê°ì„ íƒì˜µì…˜(íƒ€ì…ì œê±°)', 'ì·¨ì†Œì—¬ë¶€', 'ëª©ì ', 'ì¬êµ¬ë§¤ íšŸìˆ˜', 'ê´‘ì—­ì§€ì—­', 'ê´‘ì—­ì§€ì—­(ì •ì‹)', 'ì‹œêµ°êµ¬',
        'ê°ê·¤ ì„¸ë¶€', 'í’ˆì¢…', 'ê³¼ìˆ˜ í¬ê¸°', 'ì„ ë¬¼ì„¸íŠ¸_ì—¬ë¶€', 'ë¬´ê²Œ(kg)', 'ë¬´ê²Œ êµ¬ë¶„', 'ì´ë²¤íŠ¸ ì—¬ë¶€', 'ìƒí’ˆì„±ë“±ê¸‰_ê·¸ë£¹', 'ê°€ê²©ëŒ€'
    ]
    
    new_final = df.reindex(columns=target_columns).fillna("")
    
    for col in ['ê²°ì œê¸ˆì•¡', 'ì‹¤ê²°ì œ ê¸ˆì•¡', 'íŒë§¤ë‹¨ê°€', 'ê³µê¸‰ë‹¨ê°€']:
        new_final[col] = new_final[col].apply(lambda x: f"{int(x):,}")

    if not existing_df.empty:
        if 'ì£¼ë¬¸ì¼_temp' in existing_df.columns:
            existing_df = existing_df.drop(columns=['ì£¼ë¬¸ì¼_temp'])
        final_df = pd.concat([existing_df.reindex(columns=target_columns), new_final], ignore_index=True)
    else:
        final_df = new_final

    final_df.to_csv(LATEST_OLD_PATH, index=False, encoding='utf-8-sig')
    
    # ë²„ì „ íŒŒì¼ë¡œë„ ì €ì¥
    next_ver = get_next_version(DATA_DIR)
    ver_path = os.path.join(DATA_DIR, f"preprocessed_data_{next_ver}.csv")
    final_df.to_csv(ver_path, index=False, encoding='utf-8-sig')

    # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
    for f in new_files:
        processed_files.add(os.path.basename(f))
    save_manifest(DATA_DIR, processed_files)
    
    print(f"âœ… ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. {len(new_final):,}í–‰ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   - ìµœì¢… íŒŒì¼: {LATEST_OLD_PATH}")
    print(f"   - ë°±ì—… ë²„ì „: {os.path.basename(ver_path)}")

if __name__ == "__main__":
    rebuild_pipeline()
