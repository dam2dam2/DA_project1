import pandas as pd
import numpy as np
import os
import glob
import re
import hashlib

def generate_uid(row):
    try:
        val = str(row['íŒë§¤ë‹¨ê°€']).replace(',', '')
        unit_price = str(int(float(val)))
    except:
        unit_price = "0"
    text = f"{str(row['ìƒí’ˆì½”ë“œ'])}{str(row['ìƒí’ˆëª…'])}{unit_price}"
    return "U" + hashlib.md5(text.encode()).hexdigest().upper()[:5]

def anonymize_phone(phone_series):
    unique_phones = sorted(phone_series.unique().tolist())
    phone_map = {phone: f"ANON_{i+1:05d}" for i, phone in enumerate(unique_phones)}
    return phone_series.map(phone_map)

def get_next_version(directory, base_name="preprocessed_data"):
    files = glob.glob(os.path.join(directory, f"{base_name}_*.csv"))
    if not files:
        return 1
    
    versions = []
    for f in files:
        match = re.search(rf"{base_name}_(\d+)\.csv$", f)
        if match:
            versions.append(int(match.group(1)))
    
    return max(versions) + 1 if versions else 1

def get_latest_version_path(directory, base_name="preprocessed_data"):
    files = glob.glob(os.path.join(directory, f"{base_name}_*.csv"))
    if not files:
        # Fallback to the original preprocessed_data.csv if exists
        fallback = os.path.join(directory, "preprocessed_data.csv")
        return fallback if os.path.exists(fallback) else None
    
    versions = []
    for f in files:
        match = re.search(rf"{base_name}_(\d+)\.csv$", f)
        if match:
            versions.append((int(match.group(1)), f))
    
    if versions:
        latest_ver, latest_path = max(versions, key=lambda x: x[0])
        return latest_path
    return None

def rebuild_pipeline():
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    ORIGIN_DIR = os.path.join(BASE_DIR, "data", "origin_data")
    DATA_DIR = os.path.join(BASE_DIR, "data")
    
    next_ver = get_next_version(DATA_DIR)
    PREPROCESSED_PATH = os.path.join(DATA_DIR, f"preprocessed_data_{next_ver}.csv")
    LATEST_OLD_PATH = get_latest_version_path(DATA_DIR)

    print(f"1. ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ ë° í†µí•© ì‹œì‘: {ORIGIN_DIR}")
    excel_files = sorted(glob.glob(os.path.join(ORIGIN_DIR, "*.xlsx")))
    
    if not excel_files:
        print("Error: ì›ë³¸ ì—‘ì…€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. origin_data í´ë”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    all_dfs = []
    for f in excel_files:
        print(f"   - {os.path.basename(f)} ì½ëŠ” ì¤‘...")
        all_dfs.append(pd.read_excel(f))
    
    raw_df = pd.concat(all_dfs, ignore_index=True)
    
    # ì¤‘ë³µ ì œê±°
    if 'ì£¼ë¬¸ìƒí’ˆê³ ìœ ë²ˆí˜¸' in raw_df.columns:
        before_len = len(raw_df)
        raw_df = raw_df.drop_duplicates(subset=['ì£¼ë¬¸ìƒí’ˆê³ ìœ ë²ˆí˜¸'], keep='first')
        after_len = len(raw_df)
        if before_len > after_len:
            print(f"   - ì¤‘ë³µ ë°ì´í„° ì œê±°ë¨: {before_len - after_len}í–‰ (ì£¼ë¬¸ìƒí’ˆê³ ìœ ë²ˆí˜¸ ê¸°ì¤€)")

    print("2. ìµëª…í™” ë° ê¸°ë³¸ ì •ì œ ì§„í–‰ ì¤‘...")
    raw_df['ì£¼ë¬¸ìì—°ë½ì²˜'] = anonymize_phone(raw_df['ì£¼ë¬¸ìì—°ë½ì²˜'].astype(str))
    raw_df['ìˆ˜ë ¹ì¸ì—°ë½ì²˜'] = anonymize_phone(raw_df['ìˆ˜ë ¹ì¸ì—°ë½ì²˜'].astype(str))
    
    # ì˜µì…˜ì½”ë“œ ìƒì„±
    df_opt = raw_df[['ìƒí’ˆì½”ë“œ', 'ê³ ê°ì„ íƒì˜µì…˜']].drop_duplicates().copy()
    df_opt = df_opt.sort_values(['ìƒí’ˆì½”ë“œ', 'ê³ ê°ì„ íƒì˜µì…˜'])
    df_opt['option_idx'] = df_opt.groupby('ìƒí’ˆì½”ë“œ').cumcount() + 1
    df_opt['ì˜µì…˜ì½”ë“œ'] = df_opt['ìƒí’ˆì½”ë“œ'].astype(str) + "_" + df_opt['option_idx'].astype(str)
    raw_df = pd.merge(raw_df, df_opt[['ìƒí’ˆì½”ë“œ', 'ê³ ê°ì„ íƒì˜µì…˜', 'ì˜µì…˜ì½”ë“œ']], on=['ìƒí’ˆì½”ë“œ', 'ê³ ê°ì„ íƒì˜µì…˜'], how='left')

    mapping = {
        'ê²°ì œê¸ˆì•¡(ìƒí’ˆë³„)': 'ê²°ì œê¸ˆì•¡',
        'ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡(ìƒí’ˆë³„)': 'ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡',
        'ìƒí’ˆê¸ˆì•¡(ì˜µì…˜í¬í•¨)': 'íŒë§¤ë‹¨ê°€',
        'ê³µê¸‰ê°€': 'ê³µê¸‰ë‹¨ê°€'
    }
    df = raw_df.rename(columns=mapping).copy()

    # ìˆ«ìí˜• ë³€í™˜
    numeric_cols = ['ê²°ì œê¸ˆì•¡', 'ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡', 'íŒë§¤ë‹¨ê°€', 'ê³µê¸‰ë‹¨ê°€', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ì·¨ì†Œìˆ˜ëŸ‰']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)

    df['ì‹¤ê²°ì œ ê¸ˆì•¡'] = df['ê²°ì œê¸ˆì•¡'] - df['ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡']
    df['ì£¼ë¬¸-ì·¨ì†Œ ìˆ˜ëŸ‰'] = df['ì£¼ë¬¸ìˆ˜ëŸ‰'] - df['ì·¨ì†Œìˆ˜ëŸ‰']
    df['ì·¨ì†Œì—¬ë¶€'] = df['ì·¨ì†Œìˆ˜ëŸ‰'].apply(lambda x: 'Y' if x > 0 else 'N')
    
    # UID ìƒì„±
    df['UID'] = df.apply(generate_uid, axis=1)

    # 3. ìƒì„¸ ì „ì²˜ë¦¬ ë° íŒŒìƒ í•„ë“œ
    print("3. ìƒì„¸ ì „ì²˜ë¦¬ ë° íŒŒìƒ í•„ë“œ ìƒì„± ì¤‘...")
    df['ì£¼ë¬¸ì¼_dt'] = pd.to_datetime(df['ì£¼ë¬¸ì¼'])
    df['ì£¼ë¬¸ë‚ ì§œ'] = df['ì£¼ë¬¸ì¼_dt'].dt.date
    # ì „ì²´ ê¸°ê°„ì— ëŒ€í•œ ì¬êµ¬ë§¤ íšŸìˆ˜ ì¬ê³„ì‚° (Fresh Update)
    repurchase_map = df.groupby('ì£¼ë¬¸ìì—°ë½ì²˜')['ì£¼ë¬¸ë‚ ì§œ'].nunique().to_dict()
    df['ì¬êµ¬ë§¤ íšŸìˆ˜'] = df['ì£¼ë¬¸ìì—°ë½ì²˜'].map(lambda x: max(0, repurchase_map.get(str(x), 1) - 1))

    # ì§€ì—­ ë¶„ì„
    def extract_sido(row):
        address, postcode = str(row['ì£¼ì†Œ']), str(row['ìš°í¸ë²ˆí˜¸']).zfill(5)
        prefix = postcode[:2]
        sido_map = {'01':'ì„œìš¸','02':'ì„œìš¸','03':'ì„œìš¸','04':'ì„œìš¸','05':'ì„œìš¸','06':'ì„œìš¸','07':'ì„œìš¸','08':'ì„œìš¸','09':'ì„œìš¸','10':'ê²½ê¸°','11':'ê²½ê¸°','12':'ê²½ê¸°','13':'ê²½ê¸°','14':'ê²½ê¸°','15':'ê²½ê¸°','16':'ê²½ê¸°','17':'ê²½ê¸°','18':'ê²½ê¸°','19':'ê²½ê¸°','20':'ê²½ê¸°','21':'ì¸ì²œ','22':'ì¸ì²œ','23':'ì¸ì²œ','24':'ê°•ì›','25':'ê°•ì›','26':'ê°•ì›','27':'ì¶©ë¶','28':'ì¶©ë¶','29':'ì¶©ë¶','30':'ì„¸ì¢…','31':'ì¶©ë‚¨','32':'ì¶©ë‚¨','33':'ì¶©ë‚¨','34':'ëŒ€ì „','35':'ëŒ€ì „','36':'ê²½ë¶','37':'ê²½ë¶','38':'ê²½ë¶','39':'ê²½ë¶','40':'ê²½ë¶','41':'ëŒ€êµ¬','42':'ëŒ€êµ¬','43':'ëŒ€êµ¬','44':'ìš¸ì‚°','45':'ìš¸ì‚°','46':'ë¶€ì‚°','47':'ë¶€ì‚°','48':'ë¶€ì‚°','49':'ë¶€ì‚°','50':'ê²½ë‚¨','51':'ê²½ë‚¨','52':'ê²½ë‚¨','53':'ê²½ë‚¨','54':'ì „ë¶','55':'ì „ë¶','56':'ì „ë¶','57':'ì „ë‚¨','58':'ì „ë‚¨','59':'ì „ë‚¨','60':'ì „ë‚¨','61':'ê´‘ì£¼','62':'ê´‘ì£¼','63':'ì œì£¼'}
        formal = {'ì„œìš¸':'ì„œìš¸íŠ¹ë³„ì‹œ','ë¶€ì‚°':'ë¶€ì‚°ê´‘ì—­ì‹œ','ëŒ€êµ¬':'ëŒ€êµ¬ê´‘ì—­ì‹œ','ì¸ì²œ':'ì¸ì²œê´‘ì—­ì‹œ','ê´‘ì£¼':'ê´‘ì£¼ê´‘ì—­ì‹œ','ëŒ€ì „':'ëŒ€ì „ê´‘ì—­ì‹œ','ìš¸ì‚°':'ìš¸ì‚°ê´‘ì—­ì‹œ','ì„¸ì¢…':'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ','ê²½ê¸°':'ê²½ê¸°ë„','ê°•ì›':'ê°•ì›íŠ¹ë³„ìì¹˜ë„','ì¶©ë¶':'ì¶©ì²­ë¶ë„','ì¶©ë‚¨':'ì¶©ì²­ë‚¨ë„','ì „ë¶':'ì „ë¶íŠ¹ë³„ìì¹˜ë„','ì „ë‚¨':'ì „ë¼ë‚¨ë„','ê²½ë¶':'ê²½ìƒë¶ë„','ê²½ë‚¨':'ê²½ìƒë‚¨ë„','ì œì£¼':'ì œì£¼íŠ¹ë³„ìì¹˜ë„'}
        res = sido_map.get(prefix, next((s for s in sido_map.values() if s in address), "ê¸°íƒ€"))
        return res, formal.get(res, res)

    df[['ê´‘ì—­ì§€ì—­', 'ê´‘ì—­ì§€ì—­(ì •ì‹)']] = df.apply(lambda r: pd.Series(extract_sido(r)), axis=1)

    # í…ìŠ¤íŠ¸ ë¶„ì„
    def parse_text(row):
        txt = f"{str(row['ìƒí’ˆëª…'])} {str(row['ê³ ê°ì„ íƒì˜µì…˜'])}"
        cat = "ê°ê·¤" if any(k in txt for k in ["ê°ê·¤", "ì¡°ìƒ", "íƒ€ì´ë²¡", "ê·¤"]) else ("í™©ê¸ˆí–¥" if "í™©ê¸ˆí–¥" in txt else ("ê³ êµ¬ë§ˆ" if "ê³ êµ¬ë§ˆ" in txt else "ê¸°íƒ€"))
        sub = "íƒ€ì´ë²¡" if "íƒ€ì´ë²¡" in txt else ("ì¡°ìƒ" if "ì¡°ìƒ" in txt else ("í•˜ìš°ìŠ¤" if "í•˜ìš°ìŠ¤" in txt else "ì¼ë°˜"))
        size = "ê¸°íƒ€"
        if "ì†Œê³¼" in txt and "ì¤‘ì†Œê³¼" in txt: size = "ì†Œê³¼, ì¤‘ì†Œê³¼"
        elif "ì¤‘ëŒ€ê³¼" in txt and "ëŒ€ê³¼" in txt: size = "ì¤‘ëŒ€ê³¼, ëŒ€ê³¼"
        elif "ì†Œê³¼" in txt: size = "ì†Œê³¼"
        elif "í˜¼í•©" in txt: size = "í˜¼í•©"
        elif "ë¡œì–„ê³¼" in txt: size = "ë¡œì–„ê³¼"
        elif "ëŒ€ê³¼" in txt: size = "ëŒ€ê³¼"
        elif "ì¤‘ê³¼" in txt: size = "ì¤‘ê³¼"

        gift = "ì„ ë¬¼ì„¸íŠ¸" if any(k in txt for k in ["ì„ ë¬¼ì„¸íŠ¸", "ì„ ë¬¼ìš©"]) else "ê°€ì •ìš©"
        ev = "Y" if any(k in txt for k in ["ì´ë²¤íŠ¸", "1+1", "ë³´ì¥"]) else "N"
        grade = "í”„ë¦¬ë¯¸ì—„" if any(k in txt for k in ["í”„ë¦¬ë¯¸ì—„", "ëª…í’ˆ", "ê³ ë‹¹ë„", "íƒ€ì´ë²¡"]) else "ì¼ë°˜"

        w_match = re.findall(r'(\d+(\.\d+)?)\s*(kg|KG)', txt)
        w = sum(float(m[0]) for m in w_match) if w_match else 0.0
        grp = "<3kg" if 0 < w < 3 else ("3-5kg" if w <= 5 else ("5-10kg" if w <= 10 else ">10kg")) if w > 0 else "ë¯¸ë¶„ë¥˜"
        purpose = "ì„ ë¬¼" if str(row['ì£¼ë¬¸ìëª…']).replace(' ','') != str(row['ìˆ˜ë ¹ì¸ëª…']).replace(' ','') or "ì„ ë¬¼" in txt else "ê°œì¸ì†Œë¹„"
        
        # 'ê³ ê°ì„ íƒì˜µì…˜(íƒ€ì…ì œê±°)' ë³µêµ¬ (ì›ë³¸ì— ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê³ ê°ì„ íƒì˜µì…˜ ì‚¬ìš©)
        opt_clean = str(row.get('ê³ ê°ì„ íƒì˜µì…˜(íƒ€ì…ì œê±°)', row['ê³ ê°ì„ íƒì˜µì…˜']))

        return pd.Series([sub, cat, size, gift, w, grp, ev, grade, purpose, opt_clean])

    df[['ê°ê·¤ ì„¸ë¶€', 'í’ˆì¢…', 'ê³¼ìˆ˜ í¬ê¸°', 'ì„ ë¬¼ì„¸íŠ¸_ì—¬ë¶€', 'ë¬´ê²Œ(kg)', 'ë¬´ê²Œ êµ¬ë¶„', 'ì´ë²¤íŠ¸ ì—¬ë¶€', 'ìƒí’ˆì„±ë“±ê¸‰_ê·¸ë£¹', 'ëª©ì ', 'ê³ ê°ì„ íƒì˜µì…˜(íƒ€ì…ì œê±°)']] = df.apply(parse_text, axis=1)

    def classify_price(a):
        return "1ë§Œì› ì´í•˜" if a <= 10000 else ("1~3ë§Œì›" if a <= 30000 else ("3~5ë§Œì›" if a <= 50000 else ("5~10ë§Œì›" if a <= 100000 else "10ë§Œì› ì´ˆë°˜")))
    df['ê°€ê²©ëŒ€'] = df['ì‹¤ê²°ì œ ê¸ˆì•¡'].apply(classify_price)

    # 4. íƒ€ì… ì •ë¦¬ ë° ë¬¸ìì—´ ì „í™˜ (ì½¤ë§ˆ ì¶”ê°€)
    processed_df = df.copy() # ì›ë³¸ ë³´ì¡´ (ë¹„êµìš©)
    for col in ['ê²°ì œê¸ˆì•¡', 'ì‹¤ê²°ì œ ê¸ˆì•¡', 'íŒë§¤ë‹¨ê°€', 'ê³µê¸‰ë‹¨ê°€']:
        df[col] = df[col].apply(lambda x: f"{int(x):,}")

    target_columns = [
        'UID', 'ì£¼ë¬¸ë²ˆí˜¸', 'ì£¼ë¬¸ì¼', 'ìƒí’ˆì½”ë“œ', 'ì˜µì…˜ì½”ë“œ', 'ìƒí’ˆëª…', 'ê³ ê°ì„ íƒì˜µì…˜', 'ì£¼ë¬¸ìˆ˜ëŸ‰', 'ì·¨ì†Œìˆ˜ëŸ‰', 'ì£¼ë¬¸-ì·¨ì†Œ ìˆ˜ëŸ‰',
        'ê²°ì œê¸ˆì•¡', 'ì£¼ë¬¸ì·¨ì†Œ ê¸ˆì•¡', 'ì‹¤ê²°ì œ ê¸ˆì•¡', 'íŒë§¤ë‹¨ê°€', 'ê³µê¸‰ë‹¨ê°€', 'ì£¼ë¬¸ê²½ë¡œ', 'ì£¼ë¬¸ìëª…', 'ì…€ëŸ¬ëª…', 'ê²°ì œë°©ë²•',
        'ë°°ì†¡ì¤€ë¹„ ì²˜ë¦¬ì¼', 'ì…ê¸ˆì¼', 'ì£¼ë¬¸ìì—°ë½ì²˜', 'ì£¼ì†Œ', 'ì…ê¸ˆìëª…', 'ìˆ˜ë ¹ì¸ëª…', 'ìˆ˜ë ¹ì¸ì—°ë½ì²˜', 'íšŒì›êµ¬ë¶„', 'ìš°í¸ë²ˆí˜¸',
        'ê³ ê°ì„ íƒì˜µì…˜(íƒ€ì…ì œê±°)', 'ì·¨ì†Œì—¬ë¶€', 'ëª©ì ', 'ì¬êµ¬ë§¤ íšŸìˆ˜', 'ê´‘ì—­ì§€ì—­', 'ê´‘ì—­ì§€ì—­(ì •ì‹)',
        'ê°ê·¤ ì„¸ë¶€', 'í’ˆì¢…', 'ê³¼ìˆ˜ í¬ê¸°', 'ì„ ë¬¼ì„¸íŠ¸_ì—¬ë¶€', 'ë¬´ê²Œ(kg)', 'ë¬´ê²Œ êµ¬ë¶„', 'ì´ë²¤íŠ¸ ì—¬ë¶€', 'ìƒí’ˆì„±ë“±ê¸‰_ê·¸ë£¹', 'ê°€ê²©ëŒ€'
    ]
    
    final_df = df.reindex(columns=target_columns).fillna("")
    print(f"4. ìµœì¢… ê²°ê³¼ ì €ì¥: {PREPROCESSED_PATH}")
    final_df.to_csv(PREPROCESSED_PATH, index=False, encoding='utf-8-sig')
    
    # ìƒì„¸ ë³´ê³ 
    print("\n" + "="*50)
    print(f"ğŸ“Š ì „ì²˜ë¦¬ ì‘ì—… ê²°ê³¼ ë³´ê³  (Version {next_ver})")
    print("="*50)
    print(f"- ìƒì„± íŒŒì¼: {os.path.basename(PREPROCESSED_PATH)}")
    print(f"- ì „ì²´ ë°ì´í„° í–‰ ìˆ˜: {len(final_df):,}í–‰")
    
    if LATEST_OLD_PATH:
        print(f"- ì´ì „ íŒŒì¼ ëŒ€ë¹„ ë¹„êµ: {os.path.basename(LATEST_OLD_PATH)}")
        old_df = pd.read_csv(LATEST_OLD_PATH)
        row_diff = len(final_df) - len(old_df)
        print(f"  * ì¶”ê°€ëœ í–‰ ìˆ˜: {row_diff:+,}í–‰")
        
        # ê¸ˆì•¡ ë¹„êµ (ì½¤ë§ˆ ì œê±° í›„ ê³„ì‚°)
        def get_sum(d, col):
            return pd.to_numeric(d[col].astype(str).str.replace(',', ''), errors='coerce').sum()
        
        old_sales = get_sum(old_df, 'ì‹¤ê²°ì œ ê¸ˆì•¡')
        new_sales = get_sum(final_df, 'ì‹¤ê²°ì œ ê¸ˆì•¡')
        print(f"  * ì´ ì‹¤ê²°ì œ ê¸ˆì•¡ ë³€í™”: {old_sales:,.0f}ì› -> {new_sales:,.0f}ì› ({new_sales-old_sales:+,.0f}ì›)")
        
        # ì‹ ê·œ ì£¼ë¬¸ í™•ì¸
        old_order_ids = set(old_df['ì£¼ë¬¸ë²ˆí˜¸'].unique())
        new_order_ids = set(final_df['ì£¼ë¬¸ë²ˆí˜¸'].unique())
        added_orders = new_order_ids - old_order_ids
        if added_orders:
            print(f"  * ì‹ ê·œ ìœ ì… ì£¼ë¬¸ ìˆ˜: {len(added_orders)}ê±´")
    else:
        print("- ìµœì´ˆ ì „ì²˜ë¦¬ íŒŒì¼ ìƒì„±ì…ë‹ˆë‹¤.")
    
    print("="*50)
    print("âœ¨ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    rebuild_pipeline()
